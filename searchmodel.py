# -*- coding: utf-8 -*-
"""SearchModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GIKAAR6EXMfJvafih_b5-wWcxP6S5K0e
"""

!pip3 install scikit-learn pandas numpy matplotlib seaborn fairlearn ucimlrepo shap
!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu

import shap
import torch
import random
import numpy as np
import pandas as pd
from ucimlrepo import fetch_ucirepo
from typing import Dict, List, Tuple

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

from fairlearn.adversarial import AdversarialFairnessClassifier

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

df = pd.read_csv('amazon_output.csv')

# Calculate the average rating for each product
average_ratings = df.groupby('product_name')['rating'].mean()

# Add the average rating as a new column to the original DataFrame
df['average_rating'] = df['product_name'].map(average_ratings)

df['rating_count'] = df['rating_count'].str.replace(',', '')
df['rating_count'] = pd.to_numeric(df['rating_count'], errors='coerce')

# Add a new column to classify products as 'best seller' or 'not a best seller'
df['seller_category'] = pd.qcut(df['rating_count'], q=4, labels=['Low', 'Medium', 'High', 'Top'])

print(df.seller_category.value_counts())

df_cleaned = df.drop(columns=['product_name', 'review_id', 'review_title', 'review_content', 'product_link', 'user_name', 'user_id'])

df_cleaned['category'] = df_cleaned['category'].apply(lambda x: x.split('|')[0])

df_cleaned['discounted_price'] = df_cleaned['discounted_price'].apply(lambda x: x.replace('₹', '').replace(',', '') if isinstance(x, str) else x).astype(float)
df_cleaned['actual_price'] = df_cleaned['actual_price'].apply(lambda x: x.replace('₹', '').replace(',', '') if isinstance(x, str) else x).astype(float)

# Convert discount_percentage to numeric
df_cleaned['discount_percentage'] = df_cleaned['discount_percentage'].apply(lambda x: x.replace('%', '').replace(',', '') if isinstance(x, str) else x).astype(float) / 100

df_cleaned = df_cleaned.dropna()
# Convert rating_count to numeric
df_cleaned['rating_count'] = df_cleaned['rating_count'].apply(lambda x: x.replace(',', '') if isinstance(x, str) else x).astype(int)

# Encode 'best_seller' column
le = LabelEncoder()
df_cleaned['seller_category'] = le.fit_transform(df_cleaned['seller_category'])

# Identify numeric and categorical columns
numeric_features = ['discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count', 'sentiment', 'average_rating']
categorical_features = ['product_id', 'category']

# Create preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Split the data
train, test = train_test_split(df_cleaned, train_size=0.7, random_state=seed)

# Prepare X and y
X_train = train.drop(columns=['seller_category'])
y_train = train['seller_category']
X_test = test.drop(columns=['seller_category'])
y_test = test['seller_category']

# Fit the preprocessor on the training data and transform both training and test data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train_preprocessed.toarray(), dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)

X_test_tensor = torch.tensor(X_test_preprocessed.toarray(), dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

# Create datasets and dataloaders
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(X_train)
print(y_train)
print(X_test)
print(y_test)

seed = 42

random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

torch.use_deterministic_algorithms(True)

class MLPTabular(nn.Module):
    def __init__(self, input_size):
        super(MLPTabular, self).__init__()
        self.layer1 = nn.Linear(input_size, 64)
        self.dropout1 = nn.Dropout(0.3)
        self.layer2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.3)
        self.output_layer = nn.Linear(32, 4)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.dropout1(x)
        x = torch.relu(self.layer2(x))
        x = self.dropout2(x)
        x = self.output_layer(x)
        return x

def train_model(model, criterion, optimizer, train_loader, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        # 1. Load the inputs and labels to the training device (CPU/GPU)
        inputs = inputs.to(device)
        labels = labels.to(device).long()

        # 2. Forward Pass and loss computation
        outputs = model(inputs)
        loss_computation = criterion(outputs, labels.squeeze())

        # 3. Backward Pass
        optimizer.zero_grad()
        loss_computation.backward()
        optimizer.step()

        # Compute the loss and accuracy
        running_loss += loss_computation.item() * inputs.size(0)  # Accumulate the loss
        _, predictions = torch.max(outputs, 1)
        correct += (predictions == labels.squeeze()).sum().item()
        total += labels.size(0)

    epoch_loss = running_loss / len(train_loader.dataset)
    accuracy = correct / total
    return epoch_loss, accuracy

def evaluate_model(model, criterion, val_loader, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # 1. Load the inputs and labels to the training device (CPU/GPU)
            inputs = inputs.to(device)
            labels = labels.to(device).long()

            # 2. Forward Pass and loss computation
            outputs = model(inputs)
            loss_computation = criterion(outputs, labels.squeeze())

            # 3. Compute the validation loss and accuracy
            # Compute the loss and accuracy
            running_loss += loss_computation.item() * inputs.size(0)  # Accumulate the loss
            _, predictions = torch.max(outputs, 1)
            correct += (predictions == labels.squeeze()).sum().item()
            total += labels.size(0)

    epoch_loss = running_loss / len(val_loader.dataset)
    accuracy = correct / total
    return epoch_loss, accuracy

# Don't modify this code cell

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MLPTabular(X_train_tensor.shape[1]).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0017)

num_epochs = 1000
training_losses = []
for epoch in range(1, num_epochs + 1):
    train_loss, train_accuracy = train_model(model, criterion, optimizer, train_loader, device)
    val_loss, val_accuracy = evaluate_model(model, criterion, val_loader, device)

    if (epoch % 2) == 0:
        print(f"Epoch [{epoch}/{num_epochs}] - "
              f"Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f} - "
              f"Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f}")

    training_losses.append(train_loss)

model.eval()
with torch.no_grad():
    mlp_tabular_y_test_probs = model(X_test_tensor)
    _, test["mlp_tabular_preds"] = torch.max(mlp_tabular_y_test_probs, 1)

mlp_clf_acc = accuracy_score(test["seller_category"], test["mlp_tabular_preds"]) * 100
print(f"Accuracy of the MLP Classifier: {mlp_clf_acc:.2f}%")

n_points = int(X_test.shape[0] * 0.01)
# X_test = X_test.drop(columns=['product_id', 'category'])
X_test_sampled = shap.kmeans(X_test, k=n_points)
feature_names = X_test_df.columns.tolist()

plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss')
plt.title('Training Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Average Loss')
plt.grid(True)
plt.savefig('training_loss.png')
plt.close()

from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test, test["mlp_tabular_preds"])
sns.heatmap(cm, annot=True, fmt='d')
plt.savefig('confusion_matrix.png')
plt.close()
